# DataCloak Sentiment Workbench

A comprehensive offline-capable desktop application for processing large CSV/Excel files (up to 50GB) with automated PII masking, field inference, and sentiment analysis capabilities. Built with modern TypeScript, React, and advanced data science techniques.

## Project Structure

```
datacloak-sentiment-workbench/
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ prd/               # Product Requirements
â”‚   â”œâ”€â”€ tasks/             # Task tracking
â”‚   â”œâ”€â”€ api-contracts/     # API specifications
â”‚   â””â”€â”€ daily/             # Daily standups/updates
â”œâ”€â”€ packages/              # Main code packages
â”‚   â”œâ”€â”€ web-ui/           # âœ… React frontend (Vite + TypeScript) - COMPLETE
â”‚   â”œâ”€â”€ electron-shell/   # Electron desktop wrapper
â”‚   â”œâ”€â”€ backend/          # âœ… Express API server (SQLite + DuckDB) - COMPLETE  
â”‚   â”œâ”€â”€ datascience/      # âœ… ML & Data Science Engine - COMPLETE
â”‚   â””â”€â”€ security/         # âœ… DataCloak PII masking & security - COMPLETE
â”œâ”€â”€ shared/               # Shared resources
â”‚   â”œâ”€â”€ contracts/        # Shared TypeScript interfaces/types
â”‚   â””â”€â”€ test-fixtures/    # Test data and fixtures
â””â”€â”€ .github/workflows/    # GitHub Actions workflows
```

## ğŸ”¥ Key Features

### ğŸ§  Advanced Data Science Engine (`packages/datascience`)
- **Field Inference**: Automatically detects 13+ data types (email, URL, phone, date, JSON, etc.)
- **GPT-Powered Enhancement**: Low-confidence predictions enhanced with AI assistance
- **Pattern Recognition**: Advanced regex patterns for UUIDs, IP addresses, credit cards
- **Cost Estimation**: Accurate token counting and pricing for OpenAI/Anthropic models
- **Synthetic Data Generation**: Create realistic test datasets for users, sales, logs
- **Comprehensive Benchmarking**: Performance profiling, accuracy evaluation, memory leak detection
- **88%+ Test Coverage**: Production-ready with extensive test suites

### ğŸ”’ Enterprise Security & Privacy (`packages/security`)
- **DataCloak Integration**: Native FFI bridge + high-fidelity mock for PII detection/masking
- **Offline Processing**: Complete privacy - no data ever leaves your machine
- **OS Keychain Integration**: macOS Security framework + Windows Credential Manager
- **AES-256-CBC Encryption**: Industry-standard encryption with secure key rotation
- **Real-Time Monitoring**: 110k+ adversarial test corpus with compliance scoring
- **Security Auditing**: Comprehensive audit trails with configurable alert rules
- **Mutation Testing**: â‰¥85% mutation score for security-critical code paths
- **Cross-Platform Security**: Unified security API across Windows, macOS, Linux

### ğŸš€ Production-Ready Backend API (`packages/backend`)
- **Express TypeScript API**: RESTful endpoints for sentiment analysis and data management
- **Dual Database Architecture**: SQLite for transactions + DuckDB for analytics
- **Real Sentiment Analysis**: Keyword-based scoring with confidence metrics
- **Large File Processing**: Handle CSV/Excel files up to 50GB with streaming
- **Comprehensive Testing**: 82.1% coverage with 99+ tests (unit + integration)
- **Complete Documentation**: API reference, architecture guides, deployment docs
- **Production Ready**: Error handling, validation, logging, health monitoring

### ğŸš€ High-Performance Processing
- **Streaming Processing**: Memory-efficient chunked file processing
- **Cross-Platform**: Windows, macOS, Linux builds with code signing
- **Offline-First**: Complete privacy - no data ever leaves your machine

## Getting Started

### Prerequisites

- **Node.js** (v18+)
- **npm** (v8+)
- **10GB+ RAM** (recommended for large file processing)

### Quick Start

1. **Clone and Install**:
   ```bash
   git clone <repository-url>
   cd datacloak-sentiment-workbench
   npm install  # Installs all workspace dependencies
   ```

2. **Build All Packages**:
   ```bash
   npm run build  # Builds all packages in dependency order
   ```

3. **Development Mode**:
   ```bash
   # Terminal 1: Start backend API
   npm run dev:backend
   
   # Terminal 2: Start web UI
   npm run dev:web-ui
   
   # Terminal 3: Start Electron (optional)
   npm run dev:electron
   ```

4. **Production Build**:
   ```bash
   npm run build:electron  # Creates distributable desktop app
   ```

## ğŸ¨ Frontend Application Deep Dive (`packages/web-ui`)

The frontend is a production-ready React TypeScript application built with modern web standards and comprehensive testing.

### Frontend Architecture

```
packages/web-ui/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/           # React components (85%+ test coverage)
â”‚   â”‚   â”œâ”€â”€ WorkflowManager.tsx     # Main workflow orchestration
â”‚   â”‚   â”œâ”€â”€ DataSourcePicker.tsx    # File upload & validation
â”‚   â”‚   â”œâ”€â”€ ProfilerUI.tsx          # Data profiling & PII detection
â”‚   â”‚   â”œâ”€â”€ RunWizard.tsx           # Sentiment analysis configuration
â”‚   â”‚   â”œâ”€â”€ ResultExplorer.tsx      # Interactive results viewer
â”‚   â”‚   â”œâ”€â”€ LazyComponents.tsx      # Performance optimizations
â”‚   â”‚   â”œâ”€â”€ ErrorBoundary.tsx       # Error handling & recovery
â”‚   â”‚   â”œâ”€â”€ Navigation.tsx          # Workflow navigation
â”‚   â”‚   â””â”€â”€ FormField.tsx           # Form input components
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â””â”€â”€ AppContext.tsx          # Global state management
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ errorHandling.ts        # Error utilities & retry logic
â”‚   â”‚   â”œâ”€â”€ performance.ts          # Memory & performance monitoring
â”‚   â”‚   â””â”€â”€ validation.ts           # Form validation rules
â”‚   â”œâ”€â”€ platform-bridge.ts         # Cross-platform abstraction
â”‚   â””â”€â”€ __tests__/                  # Comprehensive test suite
â””â”€â”€ coverage/                       # Test coverage reports
```

### Key Frontend Features

**Complete Sentiment Analysis Workflow**:
1. **Data Upload**: Drag-and-drop with 50GB file support
2. **Data Profiling**: Automatic field type detection & PII identification
3. **Data Transformation**: Optional ETL with live preview
4. **Analysis Configuration**: Cost estimation & model selection
5. **Results Exploration**: Interactive charts, filtering & export

**Performance Optimizations**:
- **Lazy Loading**: Code splitting for optimal load times
- **Virtual Scrolling**: Handle millions of records efficiently
- **Memory Management**: Automatic cleanup & monitoring
- **Progressive Enhancement**: Graceful degradation support

**Cross-Platform Compatibility**:
- **Web-First Architecture**: Zero Electron dependencies in React
- **Platform Bridge**: Abstraction layer for web/desktop features
- **Universal Components**: Same code runs in browser & Electron

### Frontend Testing Architecture

**Comprehensive Test Suite** (18+ test files, 220+ tests):

```typescript
// Component testing with user interactions
describe('WorkflowManager', () => {
  it('should handle complete workflow progression', async () => {
    // Test file upload â†’ profiling â†’ analysis â†’ results
  });
});

// State management testing
describe('AppContext', () => {
  it('should manage workflow state transitions', () => {
    // Test context providers and reducers
  });
});

// Performance testing
describe('LazyComponents', () => {
  it('should handle virtual scrolling for large datasets', () => {
    // Test memory-efficient rendering
  });
});

// Error handling testing
describe('ErrorBoundary', () => {
  it('should recover from component errors gracefully', () => {
    // Test error boundaries and recovery
  });
});
```

**Test Coverage Breakdown**:
- **Components**: 12/12 major components tested (100%)
- **Utilities**: 3/3 utility modules tested (100%)
- **Context**: 1/1 state provider tested (100%)
- **Platform Layer**: Cross-platform abstraction tested
- **Error Handling**: Comprehensive error boundary testing

**Testing Technologies**:
- **Vitest**: Fast unit testing with native TypeScript
- **React Testing Library**: User-centric component testing
- **User Event**: Realistic user interaction simulation
- **Mock Service Worker**: API mocking for integration tests

### Frontend Performance Features

```typescript
// Lazy loading with preloading
const LazyComponent = createLazyComponent(
  () => import('./HeavyComponent'),
  LoadingFallback
);

// Virtual scrolling for large datasets
<VirtualizedList
  items={millionRecords}
  itemHeight={50}
  containerHeight={600}
  renderItem={MemoizedItem}
/>

// Memory monitoring
const memoryMonitor = useMemoryMonitor({
  warningThreshold: 100 * 1024 * 1024, // 100MB
  onWarning: triggerCleanup
});
```

## ğŸ§  Data Science Package Deep Dive

The `packages/datascience` directory contains our advanced ML and data processing engine:

### Architecture Overview

```
packages/datascience/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ field-inference/     # ğŸ” Type detection & pattern analysis
â”‚   â”‚   â”œâ”€â”€ engine.ts        # Main inference orchestrator
â”‚   â”‚   â”œâ”€â”€ type-detector.ts # Core type detection logic
â”‚   â”‚   â”œâ”€â”€ pattern-analyzer.ts # Regex pattern matching
â”‚   â”‚   â”œâ”€â”€ gpt-assist.ts    # AI-powered enhancement
â”‚   â”‚   â””â”€â”€ statistics-calculator.ts # Field statistics
â”‚   â”œâ”€â”€ cost-estimator/      # ğŸ’° LLM cost prediction
â”‚   â”‚   â”œâ”€â”€ estimator.ts     # Cost calculation engine
â”‚   â”‚   â”œâ”€â”€ token-counter.ts # Token estimation algorithms
â”‚   â”‚   â””â”€â”€ pricing-service.ts # Model pricing database
â”‚   â”œâ”€â”€ generators/          # ğŸ² Synthetic data creation
â”‚   â”‚   â”œâ”€â”€ data-generator.ts # High-level dataset generator
â”‚   â”‚   â”œâ”€â”€ field-generator.ts # Field-specific generators
â”‚   â”‚   â””â”€â”€ synthetic-dataset.ts # Schema-based generation
â”‚   â”œâ”€â”€ benchmarks/          # ğŸ“Š Performance & accuracy testing
â”‚   â”‚   â”œâ”€â”€ accuracy-evaluator.ts # ML metrics calculation
â”‚   â”‚   â”œâ”€â”€ performance-profiler.ts # Speed & memory profiling
â”‚   â”‚   â”œâ”€â”€ reporter.ts      # Comprehensive reporting
â”‚   â”‚   â””â”€â”€ runner.ts        # Benchmark orchestration
â”‚   â””â”€â”€ types/               # ğŸ“ TypeScript definitions
â””â”€â”€ __tests__/               # ğŸ§ª 132 tests with 88%+ coverage
```

### Field Inference Engine

**Supported Data Types** (13+ types detected):
- `string` - Generic text data
- `number` - Integers and decimals  
- `boolean` - True/false values
- `date` - Date/datetime in multiple formats
- `email` - Email addresses with validation
- `url` - Web URLs and endpoints
- `phone` - Phone numbers (international formats)
- `json` - JSON strings and objects
- `array` - Array values
- `object` - Object structures
- `null`/`undefined` - Missing values
- `mixed` - Mixed type fields

**Advanced Features**:
```typescript
import { FieldInferenceEngine } from '@datacloak/datascience';

const engine = new FieldInferenceEngine();

// Infer single field
const result = await engine.inferField('email_column', [
  'user@example.com', 'admin@test.org', 'invalid-email'
]);

// Result includes:
// - inferredType: 'email'
// - confidence: 0.67 (67% are valid emails)
// - statistics: { nullCount, uniqueCount, patterns, etc. }

// Infer entire dataset
const dataset = { 
  fields: { 
    emails: ['user@test.com'], 
    ages: [25, 30, 35] 
  } 
};
const results = await engine.inferDataset(dataset);
```

### Cost Estimation Engine

Accurate cost prediction for LLM operations:

```typescript
import { CostEstimator } from '@datacloak/datascience';

// Estimate dataset processing cost
const estimate = CostEstimator.estimateDatasetProcessing(dataset, {
  modelName: 'gpt-4',
  expectedResponseLength: 500,
  batchSize: 10
});

// Returns:
// - estimatedCost: $0.0245
// - estimatedTokens: 1250
// - confidence: 0.85
// - breakdown: { inputTokens, outputTokens, etc. }
```

**Supported Models**:
- **OpenAI**: GPT-4, GPT-4-Turbo, GPT-3.5-Turbo
- **Anthropic**: Claude-3-Opus, Claude-3-Sonnet, Claude-3-Haiku

### Synthetic Data Generation

Create realistic test datasets for development and testing:

```typescript
import { DataGenerator } from '@datacloak/datascience';

// Generate predefined datasets
const users = DataGenerator.generate({
  type: 'users',
  recordCount: 1000
});

const sales = DataGenerator.generate({
  type: 'sales', 
  recordCount: 500
});

// Custom schema generation
const custom = DataGenerator.generate({
  type: 'custom',
  recordCount: 100,
  schema: {
    id: { type: 'number', options: { minValue: 1, maxValue: 1000 } },
    name: { type: 'string', options: { minLength: 3, maxLength: 20 } },
    email: { type: 'email', options: { nullRate: 0.1 } }
  }
});
```

### Benchmarking & Performance

Comprehensive testing and profiling tools:

```typescript
import { BenchmarkRunner } from '@datacloak/datascience';

const runner = new BenchmarkRunner();

// Quick benchmark
const report = await runner.runQuickBenchmark();

// Comprehensive benchmark with accuracy and performance
const fullReport = await runner.runComprehensiveBenchmark();

// Custom benchmarks
const customReport = await runner.runBenchmark({
  name: 'Custom Test',
  testCases: [...],
  performanceTests: [...],
  accuracyThreshold: 0.9
});
```

## ğŸ”§ Backend API Deep Dive (`packages/backend`)

The backend provides a robust Express TypeScript API with comprehensive sentiment analysis and data management capabilities.

### API Architecture

```
packages/backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.ts              # Express application setup
â”‚   â”œâ”€â”€ server.ts           # Server entry point
â”‚   â”œâ”€â”€ config/             # Configuration management
â”‚   â”œâ”€â”€ controllers/        # Request handlers (92.4% coverage)
â”‚   â”‚   â”œâ”€â”€ sentiment.controller.ts # Sentiment analysis endpoints
â”‚   â”‚   â””â”€â”€ data.controller.ts      # Data management endpoints
â”‚   â”œâ”€â”€ services/           # Business logic (88.18% coverage)
â”‚   â”‚   â”œâ”€â”€ sentiment.service.ts    # Core sentiment analysis
â”‚   â”‚   â””â”€â”€ data.service.ts         # File processing & datasets
â”‚   â”œâ”€â”€ database/           # Database layer
â”‚   â”‚   â”œâ”€â”€ sqlite.ts       # SQLite connection & schemas
â”‚   â”‚   â””â”€â”€ duckdb.ts       # DuckDB analytics engine
â”‚   â”œâ”€â”€ middleware/         # Express middleware (100% coverage)
â”‚   â”œâ”€â”€ routes/             # Route definitions
â”‚   â”œâ”€â”€ validation/         # Joi schemas (100% coverage)
â”‚   â””â”€â”€ types/              # TypeScript definitions
â””â”€â”€ tests/                  # Comprehensive test suite
    â”œâ”€â”€ unit/               # Unit tests (92+ tests)
    â””â”€â”€ integration/        # API integration tests (7 tests)
```

### Key API Endpoints

**Sentiment Analysis**:
- `POST /api/v1/sentiment/analyze` - Analyze single text
- `POST /api/v1/sentiment/batch` - Batch analysis (up to 1000 texts)
- `GET /api/v1/sentiment/history` - Analysis history with pagination
- `GET /api/v1/sentiment/statistics` - Aggregate sentiment statistics

**Data Management**:
- `POST /api/v1/data/upload` - Upload CSV/Excel files (up to 50GB)
- `GET /api/v1/data/datasets` - List uploaded datasets
- `GET /api/v1/data/datasets/:id` - Get dataset details
- `DELETE /api/v1/data/datasets/:id` - Delete dataset
- `POST /api/v1/data/export` - Export analysis results

**Health & Monitoring**:
- `GET /health` - Basic health check
- `GET /api/v1/health/status` - Detailed service status

### Database Architecture

**SQLite** (Transactional Data):
```sql
-- Sentiment analysis results
CREATE TABLE sentiment_analyses (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  text TEXT NOT NULL,
  sentiment TEXT NOT NULL, -- 'positive', 'negative', 'neutral'
  score REAL NOT NULL,     -- -1.0 to 1.0
  confidence REAL NOT NULL, -- 0.0 to 1.0
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Dataset metadata
CREATE TABLE datasets (
  id TEXT PRIMARY KEY,
  filename TEXT NOT NULL,
  original_filename TEXT NOT NULL,
  size INTEGER NOT NULL,
  record_count INTEGER NOT NULL,
  mime_type TEXT,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

**DuckDB** (Analytics):
```sql
-- Extended analytics data
CREATE TABLE text_analytics (
  id INTEGER PRIMARY KEY,
  text VARCHAR,
  sentiment VARCHAR,
  score DOUBLE,
  confidence DOUBLE,
  word_count INTEGER,
  char_count INTEGER,
  batch_id VARCHAR,
  timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Backend Testing

**Test Coverage**: 82.1% overall
- **Integration Tests**: 7 passing tests (end-to-end API validation)
- **Unit Tests**: 92+ passing tests
- **Controllers**: 92.4% coverage
- **Services**: 88.18% coverage  
- **Middleware**: 100% coverage
- **Validation**: 100% coverage

```bash
# Run backend tests
cd packages/backend
npm test                # All tests
npm run test:coverage   # With coverage report
npm run test:integration # Integration tests only
```

### Production Deployment

```bash
# Build for production
npm run build

# Start production server
npm start

# Environment variables
PORT=3001
NODE_ENV=production
SQLITE_DB_PATH=./data/production.db
DUCKDB_PATH=./data/analytics.db
```

## ğŸ“Š Testing & Quality

### Test Coverage Summary
- **Backend API**: **82.1%** overall coverage (99+ tests)
- **Data Science**: **88.01%** statement coverage (132+ tests)
- **Security**: **100%** coverage (57+ tests)
- **Frontend**: **85%+** statement coverage (18+ test files, 220+ tests)

**Package-by-Package Coverage**:
- **Backend Controllers**: 92.4%
- **Backend Services**: 88.18%
- **Backend Middleware**: 100%
- **Field Inference**: 90.84%
- **Cost Estimator**: 96.33%
- **Data Generators**: 98.8%
- **Security Modules**: 100%

### Running Tests

```bash
# Run all tests
npm test

# Run tests with coverage
npm run test -- --coverage

# Run data science tests specifically
npm run test --workspace=packages/datascience

# Run benchmarks
npm run benchmark
npm run benchmark comprehensive
```

## ğŸš€ Development Workflow

### Package Scripts

```bash
# Development
npm run dev              # Start backend + frontend
npm run dev:web-ui       # Frontend only  
npm run dev:backend      # Backend only
npm run dev:electron     # Electron shell

# Building
npm run build           # Build all packages
npm run build:web-ui    # Frontend build
npm run build:backend   # Backend build  
npm run build:datascience # Data science build
npm run build:electron  # Electron distributables

# Testing
npm run test           # All tests
npm run test:unit      # Unit tests only
npm run test:integration # Integration tests
npm run test:e2e       # End-to-end tests
npm run test:mutation  # Mutation testing

# Quality & Deployment
npm run lint           # ESLint all packages
npm run typecheck      # TypeScript validation
npm run coverage:merge # Merge coverage reports
npm run security:check # Security audit
```

## Contributing

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
